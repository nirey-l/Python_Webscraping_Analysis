{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b63e3",
   "metadata": {},
   "source": [
    "### 1. Daum 뉴스 기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90268913",
   "metadata": {},
   "source": [
    "질문1. 아래의 url에서 뉴스 기사의 링크와 제목을 출력하시오\n",
    "*  경제 뉴스 url = 'https://news.daum.net/economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cd7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/economy\n",
      "<class 'requests.models.Response'>\n",
      "200\n",
      "<class 'bs4.element.ResultSet'> 9\n",
      "https://v.daum.net/v/20260206213503000 제주항공은 비행기를 늘리지 않았다… 고환율 시대, LCC가 버티는 법을 먼저  정했다 제주항공이 차세대 항공기 B737-8 9호기를 구매 방식으로 도입했습니다.  표면적으로는 ‘기재 한 대’추가입니다. 하지만 이 선택은 국내 저비용항공사(LCC) 경쟁의 문법을 바꾸는 결정에 가깝습니다. 고환율이 더 이상 일시적 변수가 아닌 구조로 굳어진 상황에서, 항공사의 성패는 더 많이 띄우는 능력이 아니라 달러 비용을 얼마나 구조적으로 통제하느냐로 갈 JIBS 2시간 전\n",
      "https://v.daum.net/v/20260206210724599 '美 정부 안전망 없다' 충격…비트코인 6만달러선까지 후퇴·이더리움도 동반 흔들 [이수현의 코인레이더] 한국경제 3시간 전\n",
      "https://v.daum.net/v/20260206202727940 정책자금 브로커 구조 손본다…중기부, 서류 간소화도 추진 KBS 3시간 전\n",
      "https://v.daum.net/v/20260206185614120 휴머노이드 상용화 ‘초읽기’…“안전성·신뢰성이 최우선” 한겨레 5시간 전\n",
      "https://v.daum.net/v/20260206184326845 EV 부진에 배터리서 발 빼는 美 완성차… LG에너지솔루션, 스텔란티스 합작법인 인수 동아일보 5시간 전\n",
      "https://v.daum.net/v/20260206172807685 키키도 소환한 그 감성⋯Y2K, 왜 아직도 먹히냐면요 [솔드아웃] 이투데이 6시간 전\n",
      "https://v.daum.net/v/20260206162540251 2026 지금은 크립토 윈터인가? 규제 이후 시장의 변화 [타이거리서치 리포트] 한국경제 7시간 전\n",
      "https://v.daum.net/v/20260206160226448 1년새 2900% '폭풍 성장'…\"주식 토큰화, '코스닥 3000' 만들 성장동력\" 한국경제 8시간 전\n",
      "https://v.daum.net/v/20260206153503273 새벽은 풀고, 일요일 묶어버리면… 대형마트 새벽배송 논의, 지표가 먼저 보여준 결과 JIBS 8시간 전\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# dict 타입으로 요청 파라미터 설정\n",
    "# 요청 파라미터 설정: 고정된 주소 뒤에 내가 원하는 조건(카테고리 등)을 유동적으로 조립하기 위해 미리 값을 준비해두는 과정\n",
    "req_param = {\n",
    "    'category': 'economy' \n",
    "}\n",
    "\n",
    "url = 'https://news.daum.net/{category}'.format(**req_param)\n",
    "print(url) \n",
    "\n",
    "# 요청 헤더 설정 why? 프로그램이 아닌 사람처럼 보이게 하기 위함\n",
    "# 개발자 도구 네트워크 Doc의 헤더에서 가져올 수 있음\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests의 get() 함수 호출하기\n",
    "# requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "# url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "# headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "print(type(res)) \n",
    "print(res.status_code) \n",
    "\n",
    "# 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "if res.ok:\n",
    "    # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "    # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "    soup = BeautifulSoup(res.text, 'html.parser') \n",
    "\n",
    "    # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "    # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "    # a.item_newsheadline2: 클래스가 item_newsheadline2인 a 태그들만 찾기\n",
    "    ul_tags = soup.select(\"ul.list_newsheadline2 a.item_newsheadline2\")\n",
    "\n",
    "    # type: 데이터 형식\n",
    "    # len: 그 안에 든 뉴스 기사가 몇 개인지\n",
    "    print(type(ul_tags), len(ul_tags))\n",
    "\n",
    "    # 찾아온 뉴스 꾸러미(ul_tags)에서 기사를 하나씩 꺼내서(ul_tag) 아래 작업을 반복\n",
    "    for ul_tag in ul_tags:\n",
    "\n",
    "        # <ul>...</ul> 태그 사이에 적힌 글자(뉴스 제목)만 가져오기\n",
    "        # .text: 태그 안의 텍스트만 가져오기\n",
    "        # .strip(): 공백, 줄바꿈 제거\n",
    "        title = ul_tag.text.strip() \n",
    "\n",
    "        # 해당하는 링크 가져오기\n",
    "        link = ul_tag['href']\n",
    "\n",
    "        print(link, title)\n",
    "\n",
    "else:\n",
    "    # 응답(response)이 Error이면 status code 출력\n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd751",
   "metadata": {},
   "source": [
    "질문2. 여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n",
    "*   경제 뉴스 url = 'https://news.daum.net/economy'\n",
    "*   사회 뉴스 url = 'https://news.daum.net/society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1673e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics', '국제':'world','문화':'culture',\n",
    "                '생활':'life','IT/과학':'tech','인물':'people','지식/칼럼':'understanding','연재':'series'}\n",
    "\n",
    "def print_news(section_name):\n",
    "    # 정치: section_dict에 없을 때를 대비한 기본값  \n",
    "    sid = section_dict.get(section_name, '정치')\n",
    "    url = f'https://news.daum.net/{sid}'\n",
    "    # f-string: 문자열 안에서 중괄호 { }를 사용해 변수나 계산식을 직접 넣을 수 있음\n",
    "    # print(\"======>\" + url + section_name + \"뉴스\" + \"<======\")\n",
    "    print(f'======> {url} {section_name} 뉴스 <======')\n",
    "\n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    # if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "    if res.ok:\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "        # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "        # a.item_newsheadline2: 클래스가 item_newsheadline2인 a 태그들만 찾기\n",
    "        ul_tags = soup.select(\"ul.list_newsheadline2 a.item_newsheadline2\")\n",
    "\n",
    "        # 찾아온 뉴스 꾸러미(ul_tags)에서 기사를 하나씩 꺼내서(ul_tag) 아래 작업을 반복\n",
    "        for ul_tag in ul_tags:\n",
    "\n",
    "            # <ul>...</ul> 태그 사이에 적힌 글자(뉴스 제목)만 가져오기\n",
    "            # .text: 태그 안의 텍스트만 가져오기\n",
    "            # .strip(): 공백, 줄바꿈 제거\n",
    "            title = ul_tag.text.strip() \n",
    "\n",
    "            # 해당하는 링크 가져오기\n",
    "            link = ul_tag['href']\n",
    "\n",
    "            print(link, title)\n",
    "\n",
    "    else:\n",
    "            # 응답(response)이 Error이면 status code 출력\n",
    "            print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc1fc",
   "metadata": {},
   "source": [
    "### 2-1. Nate 뉴스 기사 제목 스크래핑하기\n",
    "*   https://news.nate.com/recent?mid=n0100\n",
    "*   최신뉴스, 정치 , 경제, 사회, 세계, IT/과학 6개의 섹션의 뉴스를 출력하는 함수를 생성하여 스크래핑 하기\n",
    "*   뉴스기사의 Image를 출력 하세요\n",
    "*   Image의 도메인이름이 포함된 url과 src 속성의 img 경로를 합치려면 urljoin 함수를 사용하세요\n",
    "*   Image 출력은 Image 클래스와 display 함수를 사용하세요\n",
    "*   img 엘리먼트의 존재 여부를 체크하신 후에 src 속성의 이미지를 경로를 추출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500e813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade47db3",
   "metadata": {},
   "source": [
    "### 2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차번호(no),회차의URL(url) 을 입력으로 받는 함수를 선언\n",
    "*   아래와 같이 호출\n",
    "\n",
    "    ```download_one_episode('일렉시드',341,'https://comic.naver.com/webtoon/detail?titleId=717481&no=341&week=wed')```\n",
    "    \n",
    "*   img\\일렉시드\\341 디렉토리가 생성되며, 그 디렉토리 아래에 웹툰 image들이 다운로드 되도록 해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e799d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318fe264",
   "metadata": {},
   "source": [
    "### 2-3. 하나의 네이버 웹툰과 여러개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차를 알 수 있는 url을 입력으로 받는 함수를 선언\n",
    "\n",
    "    ````def download_all_episode(title, episode_url):````\n",
    "\n",
    "*   하나의 웹툰에 대한 1Page의 20 회차의 image를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4992ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8da8c04",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
