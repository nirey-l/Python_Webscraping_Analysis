{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b63e3",
   "metadata": {},
   "source": [
    "### 1. Daum 뉴스 기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90268913",
   "metadata": {},
   "source": [
    "질문1. 아래의 url에서 뉴스 기사의 링크와 제목을 출력하시오\n",
    "*  경제 뉴스 url = 'https://news.daum.net/economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# dict 타입으로 요청 파라미터 설정\n",
    "# 요청 파라미터 설정: 고정된 주소 뒤에 내가 원하는 조건(카테고리 등)을 유동적으로 조립하기 위해 미리 값을 준비해두는 과정\n",
    "req_param = {\n",
    "    'category': 'economy' \n",
    "}\n",
    "\n",
    "url = 'https://news.daum.net/{category}'.format(**req_param)\n",
    "print(url) \n",
    "\n",
    "# 요청 헤더 설정 why? 프로그램이 아닌 사람처럼 보이게 하기 위함\n",
    "# 개발자 도구 네트워크 Doc의 헤더에서 가져올 수 있음\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests의 get() 함수 호출하기\n",
    "# requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "# url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "# headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "print(type(res)) \n",
    "print(res.status_code) \n",
    "\n",
    "# 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "if res.ok:\n",
    "    # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "    # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "    soup = BeautifulSoup(res.text, 'html.parser') \n",
    "\n",
    "    # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "    # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "    # a.item_newsheadline2: 클래스가 item_newsheadline2인 a 태그들만 찾기\n",
    "    ul_tags = soup.select(\"ul.list_newsheadline2 a.item_newsheadline2\")\n",
    "\n",
    "    # type: 데이터 형식\n",
    "    # len: 그 안에 든 뉴스 기사가 몇 개인지\n",
    "    print(type(ul_tags), len(ul_tags))\n",
    "\n",
    "    # 찾아온 뉴스 꾸러미(ul_tags)에서 기사를 하나씩 꺼내서(ul_tag) 아래 작업을 반복\n",
    "    for ul_tag in ul_tags:\n",
    "\n",
    "        # <ul>...</ul> 태그 사이에 적힌 글자(뉴스 제목)만 가져오기\n",
    "        # .text: 태그 안의 텍스트만 가져오기\n",
    "        # .strip(): 공백, 줄바꿈 제거\n",
    "        title = ul_tag.text.strip() \n",
    "\n",
    "        # 해당하는 링크 가져오기\n",
    "        link = ul_tag['href']\n",
    "\n",
    "        print(link, title)\n",
    "\n",
    "else:\n",
    "    # 응답(response)이 Error이면 status code 출력\n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd751",
   "metadata": {},
   "source": [
    "질문2. 여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n",
    "*   경제 뉴스 url = 'https://news.daum.net/economy'\n",
    "*   사회 뉴스 url = 'https://news.daum.net/society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1673e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics', '국제':'world','문화':'culture',\n",
    "                '생활':'life','IT/과학':'tech','인물':'people','지식/칼럼':'understanding','연재':'series'}\n",
    "\n",
    "def print_news(section_name):\n",
    "    # 정치: section_dict에 없을 때를 대비한 기본값  \n",
    "    sid = section_dict.get(section_name, '정치')\n",
    "    url = f'https://news.daum.net/{sid}'\n",
    "    # f-string: 문자열 안에서 중괄호 { }를 사용해 변수나 계산식을 직접 넣을 수 있음\n",
    "    # print(\"======>\" + url + section_name + \"뉴스\" + \"<======\")\n",
    "    print(f'======> {url} {section_name} 뉴스 <======')\n",
    "\n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    # if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "    if res.ok:\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "        # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "        # a.item_newsheadline2: 클래스가 item_newsheadline2인 a 태그들만 찾기\n",
    "        ul_tags = soup.select(\"ul.list_newsheadline2 a.item_newsheadline2\")\n",
    "\n",
    "        # 찾아온 뉴스 꾸러미(ul_tags)에서 기사를 하나씩 꺼내서(ul_tag) 아래 작업을 반복\n",
    "        for ul_tag in ul_tags:\n",
    "\n",
    "            # <ul>...</ul> 태그 사이에 적힌 글자(뉴스 제목)만 가져오기\n",
    "            # .text: 태그 안의 텍스트만 가져오기\n",
    "            # .strip(): 공백, 줄바꿈 제거\n",
    "            title = ul_tag.text.strip() \n",
    "\n",
    "            # 해당하는 링크 가져오기\n",
    "            link = ul_tag['href']\n",
    "\n",
    "            print(link, title)\n",
    "\n",
    "    else:\n",
    "            # 응답(response)이 Error이면 status code 출력\n",
    "            print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc1fc",
   "metadata": {},
   "source": [
    "### 2-1. Nate 뉴스 기사 제목 스크래핑하기\n",
    "*   https://news.nate.com/recent?mid=n0100\n",
    "*   최신뉴스, 정치 , 경제, 사회, 세계, IT/과학 6개의 섹션의 뉴스를 출력하는 함수를 생성하여 스크래핑 하기\n",
    "*   뉴스기사의 Image를 출력 하세요\n",
    "*   Image의 도메인이름이 포함된 url과 src 속성의 img 경로를 합치려면 urljoin 함수를 사용하세요\n",
    "*   Image 출력은 Image 클래스와 display 함수를 사용하세요\n",
    "*   img 엘리먼트의 존재 여부를 체크하신 후에 src 속성의 이미지를 경로를 추출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9500e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import Image, display\n",
    "\n",
    "section_dict = {'최신뉴스':100,'정치':200, '경제':300, '사회':400 ,'세계':500, 'IT/과학':600}\n",
    "\n",
    "def print_news(section_name):\n",
    "    # 정치: section_dict에 없을 때를 대비한 기본값  \n",
    "    sid = section_dict.get(section_name, '정치')\n",
    "    url = f'https://news.nate.com/recent?mid=n0{sid}'\n",
    "\n",
    "    print(f\"======> {section_name} 뉴스 <======\")\n",
    "    \n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    # if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "    if res.ok:\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        articles = soup.select('.newsList > ul > li')\n",
    "\n",
    "        for article in articles:\n",
    "\n",
    "            # 이미지 출력\n",
    "            img_tag = article.select_one('img')\n",
    "            if img_tag and 'src' in img_tag.attrs:\n",
    "                img_url = urljoin(url, img_tag['src'])\n",
    "                display(Image(url=img_url, width=150))\n",
    "            else:\n",
    "                print(\"(이미지 없음)\") \n",
    "\n",
    "            # 기사 제목 및 링크\n",
    "            a_tag = article.select_one('a')\n",
    "            if a_tag:\n",
    "                # 제목 추출 (.title 클래스 사용)\n",
    "                title = a_tag.select_one('.title').text.strip() if a_tag.select_one('.title') else \"제목 없음\"\n",
    "                # 링크 추출\n",
    "                link = urljoin(url, a_tag['href'])\n",
    "                \n",
    "                print(f\"기사제목: {title}\")\n",
    "                print(f\"기사링크: {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1feb7a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> 경제 뉴스 <======\n"
     ]
    }
   ],
   "source": [
    "print_news('경제')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade47db3",
   "metadata": {},
   "source": [
    "### 2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차번호(no),회차의URL(url) 을 입력으로 받는 함수를 선언\n",
    "*   아래와 같이 호출\n",
    "\n",
    "    ```download_one_episode('일렉시드',341,'https://comic.naver.com/webtoon/detail?titleId=717481&no=341&week=wed')```\n",
    "    \n",
    "*   img\\일렉시드\\341 디렉토리가 생성되며, 그 디렉토리 아래에 웹툰 image들이 다운로드 되도록 해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e799d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318fe264",
   "metadata": {},
   "source": [
    "### 2-3. 하나의 네이버 웹툰과 여러개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차를 알 수 있는 url을 입력으로 받는 함수를 선언\n",
    "\n",
    "    ````def download_all_episode(title, episode_url):````\n",
    "\n",
    "*   하나의 웹툰에 대한 1Page의 20 회차의 image를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4992ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8da8c04",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
