{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b63e3",
   "metadata": {},
   "source": [
    "### 1. Daum 뉴스 기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90268913",
   "metadata": {},
   "source": [
    "질문1. 아래의 url에서 뉴스 기사의 링크와 제목을 출력하시오\n",
    "*  경제 뉴스 url = 'https://news.daum.net/economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# dict 타입으로 요청 파라미터 설정\n",
    "# 요청 파라미터 설정: 고정된 주소 뒤에 내가 원하는 조건(카테고리 등)을 유동적으로 조립하기 위해 미리 값을 준비해두는 과정\n",
    "req_param = {\n",
    "    'category': 'economy' \n",
    "}\n",
    "\n",
    "url = 'https://news.daum.net/{category}'.format(**req_param)\n",
    "print(url) \n",
    "\n",
    "# 요청 헤더 설정 why? 프로그램이 아닌 사람처럼 보이게 하기 위함\n",
    "# 개발자 도구 네트워크 Doc의 헤더에서 가져올 수 있음\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests의 get() 함수 호출하기\n",
    "# requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "# url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "# headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "print(type(res)) \n",
    "print(res.status_code) \n",
    "\n",
    "# if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "if res.ok: \n",
    "    # 한글 깨짐 방지\n",
    "    res.encoding = 'utf-8'\n",
    "    # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "    # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "    soup = BeautifulSoup(res.text, 'html.parser') \n",
    "\n",
    "    # 기사 제목과 링크 추출\n",
    "    # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "    # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "    li_tag_list = soup.select(\"ul.list_newsheadline2 li\")\n",
    "\n",
    "    # type: 데이터 형식\n",
    "    # len: 그 안에 든 뉴스 기사가 몇 개인지\n",
    "    print(type(li_tag_list), len(li_tag_list))\n",
    "\n",
    "    # 찾아온 뉴스 꾸러미(li_tag_list)에서 기사를 하나씩 꺼내서(li_tag) 아래 작업을 반복\n",
    "    for li_tag in li_tag_list:\n",
    "\n",
    "        a_tag = li_tag.find('a')\n",
    "        # 해당하는 링크를 가져와서 출력하기\n",
    "        print(a_tag['href'])\n",
    "\n",
    "        #strong_tag = li_tag.select('div.cont_thumb strong.tit_txt')[0]\n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "        # .text: 태그 안의 텍스트만 가져오기\n",
    "        # .strip(): 공백, 줄바꿈 제거\n",
    "        title = strong_tag.text.strip()\n",
    "        print(title)\n",
    "\n",
    "else:\n",
    "    # 응답(response)이 Error이면 status code 출력\n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd751",
   "metadata": {},
   "source": [
    "질문2. 여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n",
    "*   경제 뉴스 url = 'https://news.daum.net/economy'\n",
    "*   사회 뉴스 url = 'https://news.daum.net/society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1673e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics', '국제':'world','문화':'culture',\n",
    "                '생활':'life','IT/과학':'tech','인물':'people','지식/칼럼':'understanding','연재':'series'}\n",
    "\n",
    "def print_news(section_name):\n",
    "\n",
    "    section = section_dict[section_name]\n",
    "\n",
    "    # 요청 Parameter\n",
    "    req_param = {\n",
    "        'section': section\n",
    "    }\n",
    "\n",
    "    url = 'https://news.daum.net/{section}'.format(**req_param)\n",
    "\n",
    "    # f-string: 문자열 안에서 중괄호 { }를 사용해 변수나 계산식을 직접 넣을 수 있음\n",
    "    # print(\"======>\" + url + section_name + \"뉴스\" + \"<======\")\n",
    "    print(f'======> {url} {section_name} 뉴스 <======')\n",
    "\n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "    \n",
    "    # if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "    if res.ok:\n",
    "        # 한글 깨짐 방지\n",
    "        res.encoding = 'utf-8'\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 기사 제목과 링크 추출\n",
    "        # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "        # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "        li_tag_list = soup.select(\"ul.list_newsheadline2 li\")\n",
    "\n",
    "        # 찾아온 뉴스 꾸러미(li_tag_list)에서 기사를 하나씩 꺼내서(li_tag) 아래 작업을 반복\n",
    "        for li_tag in li_tag_list:\n",
    "\n",
    "            a_tag = li_tag.find('a')\n",
    "            # 해당하는 링크를 가져와서 출력하기\n",
    "            print(a_tag['href'])\n",
    "\n",
    "            #strong_tag = li_tag.select('div.cont_thumb strong.tit_txt')[0]\n",
    "            strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "            # .text: 태그 안의 텍스트만 가져오기\n",
    "            # .strip(): 공백, 줄바꿈 제거\n",
    "            title = strong_tag.text.strip()\n",
    "            print(title)\n",
    "\n",
    "    else:\n",
    "        # 응답(response)이 Error이면 status code 출력\n",
    "        print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc1fc",
   "metadata": {},
   "source": [
    "### 2-1. Nate 뉴스 기사 제목 스크래핑하기\n",
    "*   https://news.nate.com/recent?mid=n0100\n",
    "*   최신뉴스, 정치 , 경제, 사회, 세계, IT/과학 6개의 섹션의 뉴스를 출력하는 함수를 생성하여 스크래핑 하기\n",
    "*   뉴스기사의 Image를 출력 하세요\n",
    "*   Image의 도메인이름이 포함된 url과 src 속성의 img 경로를 합치려면 urljoin 함수를 사용하세요\n",
    "*   Image 출력은 Image 클래스와 display 함수를 사용하세요\n",
    "*   img 엘리먼트의 존재 여부를 체크하신 후에 src 속성의 이미지를 경로를 추출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9500e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import Image, display\n",
    "\n",
    "section_dict = {'최신뉴스':100,'정치':200, '경제':300, '사회':400 ,'세계':500, 'IT/과학':600}\n",
    "\n",
    "def print_news(section_num):\n",
    "\n",
    "    section = section_dict[section_num]\n",
    "\n",
    "    # 요청 Parameter\n",
    "    req_param = {\n",
    "        'section': section\n",
    "    }\n",
    "\n",
    "    url = 'https://news.nate.com/recent?mid=n0{section}'.format(**req_param)\n",
    "    \n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    if res.ok:\n",
    "        # 한글 깨짐 방지\n",
    "        res.encoding = 'euc-kr'\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 기사 묶음 리스트\n",
    "        tags = soup.select(\"div.postListType.noListTitle div.mlt01\")\n",
    "\n",
    "        # idx: 기사 번호\n",
    "        # div_tag: 뉴스 기사 하나에 해당하는 html 덩어리\n",
    "        # enumerate(tags, 1): tags에서 하나식 꺼내는데 번호표를 1번부터 붙여서 가져옴\n",
    "        for idx, div_tag in enumerate(tags, 1): \n",
    "            print(f'============>> {idx}')\n",
    "\n",
    "            # 기사 덩어리 안에서 링크가 걸린 <a>태그 찾기       \n",
    "            a_tag = div_tag.find('a')\n",
    "            # urljoin(url, ...): 생략된 앞부분 주소를 자동으로 붙여서 완전한 주소로 만들어주는 함수\n",
    "            # 현재 <a>태그에 있는 링크: //news.nate.com/view/20260209n25017?mid=n0100(불완전한 기사 주소)\n",
    "            a_join_url = urljoin(url, a_tag['href'])\n",
    "            print(f'뉴스기사 링크 = {a_join_url}')\n",
    "\n",
    "            # span.ib 안에 있는 이미지를 찾기\n",
    "            img_tag = div_tag.select_one('span.ib img')\n",
    "            # 이미지가 있을 때만 실행하기\n",
    "            if img_tag:\n",
    "                photo_url = urljoin(url, img_tag['src'])\n",
    "                # 웹사이트 소스에서 갓 긁어온 값 (불완전한 주소)\n",
    "                print(img_tag['src'])\n",
    "                # urljoin을 걸쳐서 완벽한 절대주소 값 (완전한 이미지 주소)\n",
    "                print(photo_url)\n",
    "                # 텍스트 주소 형태인 photo_url을 실제 이미지로 변환해서 화면에 보여줌\n",
    "                display(Image(url=photo_url))\n",
    "\n",
    "            # 기사 제목이 들어있는 <h2>태그만\n",
    "            h2_tag = div_tag.select_one('span.tb h2.tit')\n",
    "            # .text: <h2>...</h2>태그는 버리고 그 사이에 적힌 제목만\n",
    "            title = h2_tag.text\n",
    "            print(title)\n",
    "\n",
    "    else:\n",
    "        # 응답(response)이 Error이면 status code 출력\n",
    "        print(f'Error Code = {res.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade47db3",
   "metadata": {},
   "source": [
    "### 2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차번호(no),회차의URL(url) 을 입력으로 받는 함수를 선언\n",
    "*   아래와 같이 호출\n",
    "\n",
    "    ```download_one_episode('일렉시드',341,'https://comic.naver.com/webtoon/detail?titleId=717481&no=341&week=wed')```\n",
    "    \n",
    "*   img\\일렉시드\\341 디렉토리가 생성되며, 그 디렉토리 아래에 웹툰 image들이 다운로드 되도록 해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e799d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318fe264",
   "metadata": {},
   "source": [
    "### 2-3. 하나의 네이버 웹툰과 여러개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차를 알 수 있는 url을 입력으로 받는 함수를 선언\n",
    "\n",
    "    ````def download_all_episode(title, episode_url):````\n",
    "\n",
    "*   하나의 웹툰에 대한 1Page의 20 회차의 image를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4992ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "839c8ee4",
   "metadata": {},
   "source": [
    "### 3-1. 네이버 책 검색 API 호출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b276abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Do it! 점프 투 파이썬 (중학생도 첫날부터 실습하는 초고속 입문서)', 'link': 'https://search.shopping.naver.com/book/catalog/40354085633', 'image': 'https://shopping-phinf.pstatic.net/main_4035408/40354085633.20230927071024.jpg', 'author': '박응용', 'discount': '19800', 'publisher': '이지스퍼블리싱', 'pubdate': '20230615', 'isbn': '9791163034735', 'description': '프로그래밍 분야 8년 연속 베스트셀러!\\n《Do it! 점프 투 파이썬》 전면 개정 2판 출시!\\n\\n중고등학생도, 비전공자도, 직장인도 프로그래밍에 눈뜨게 만든 바로 그 책이 전면 개정 2판으로 새롭게 태어났다! 챗GPT를 시작으로 펼쳐진 생성 AI 시대에 맞춰 설명과 예제를 다듬고, 최신 경향과 심화 내용을 보충했다. 또한 이번 개정 2판도 50만 코딩 유튜버인 조코딩과 협업을 통해 유튜브 동영상을 제공해 파이썬을 더 쉽게 공부할 수 있다.\\n\\n8년 연속 베스트셀러! 위키독스 누적 방문 300만! 독자의 입에서 입으로 전해진 추천과 수많은 대학 및 학원의 교재 채택을 통해 검증은 이미 끝났다. 코딩을 처음 배우는 중고등학생부터 코딩 소양을 기르려는 비전공자, 자기계발에 진심인 직장인까지! 이 책과 함께 파이썬 프로그래밍의 세계로 ‘점프’해 보자!'}, {'title': '혼자 공부하는 파이썬 (1:1 과외하듯 배우는 프로그래밍 자습서)', 'link': 'https://search.shopping.naver.com/book/catalog/32507605957', 'image': 'https://shopping-phinf.pstatic.net/main_3250760/32507605957.20230509170119.jpg', 'author': '윤인성', 'discount': '19800', 'publisher': '한빛미디어', 'pubdate': '20220601', 'isbn': '9791162245651', 'description': '혼자 해도 충분하다! 1:1 과외하듯 배우는 파이썬 프로그래밍 자습서\\n\\n『혼자 공부하는 파이썬』이 더욱 흥미있고 알찬 내용으로 개정되었습니다. 프로그래밍이 정말 처음인 입문자도 따라갈 수 있는 친절한 설명과 단계별 학습은 그대로! 혼자 공부하더라도 체계적으로 계획을 세워 학습할 수 있도록 ‘혼공 계획표’를 새롭게 추가했습니다. 또한 입문자가 자주 물어보는 질문과 오류 해결 방법을 적재적소에 배치하여 예상치 못한 문제에 부딪혀도 좌절하지 않고 끝까지 완독할 수 있도록 도와줍니다. 단순한 문법 암기와 코딩 따라하기에 지쳤다면, 새로운 혼공파와 함께 ‘누적 예제’와 ‘도전 문제’로 프로그래밍의 신세계를 경험해 보세요! 배운 내용을 씹고 뜯고 맛보고 즐기다 보면 응용력은 물론 알고리즘 사고력까지 키워 코딩 실력이 쑥쑥 성장할 것입니다.\\n\\n이 책은 독학으로 파이썬을 배우는 입문자가 ‘꼭 필요한 내용을 제대로 학습’할 수 있도록 구성했습니다. 뭘 모르는지조차 모르는 입문자의 막연한 마음에 십분 공감하여 과외 선생님이 알려주듯 친절하게, 핵심적인 내용만 콕콕 집어줍니다. 책의 첫 페이지를 펼쳐서 마지막 페이지를 덮을 때까지, 혼자서도 충분히 파이썬을 배울 수 있다는 자신감과 확신이 계속될 것입니다!\\n\\n베타리더와 함께 입문자에게 맞는 난이도, 분량, 학습 요소 등을 적극 반영했습니다. 어려운 용어와 개념은 한 번 더 풀어쓰고, 복잡한 설명은 눈에 잘 들어오는 그림으로 풀어냈습니다. ‘혼자 공부해 본’ 여러 입문자의 초심과 눈높이가 책 곳곳에 반영된 것이 이 책의 가장 큰 장점입니다.'}]\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 현재 폴더에 있는 .env 파일을 찾아 내용을 메모리 불러오기\n",
    "load_dotenv()\n",
    "# os.getenv()를 통해 .env 안에 적힌 '변수명'으로 실제 값 가져오기\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "headers = {\n",
    "    'X-Naver-Client-Id': CLIENT_ID,\n",
    "    'X-Naver-Client-Secret': CLIENT_SECRET\n",
    "}\n",
    "\n",
    "def search_books(query):\n",
    "\n",
    "    # query string 문자열을 dict로 선언 \n",
    "    payload = {\n",
    "        'query': query, # 파이썬\n",
    "        'display': 100,\n",
    "        'sort': 'sim'\n",
    "    }\n",
    "\n",
    "    url = 'https://openapi.naver.com/v1/search/book.json'\n",
    "\n",
    "    # API 요청 \n",
    "    # requests get(url, params, headers) 요청\n",
    "    res = requests.get(url, params=payload, headers=headers)\n",
    "\n",
    "    if res.ok:\n",
    "        # json()함수로 응답 결과 가져오기\n",
    "        return res.json()['items']\n",
    "    else:\n",
    "        print(f\"Error: {res.status_code}\")\n",
    "        return []\n",
    "    \n",
    "items_data = search_books('파이썬')\n",
    "print(items_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e8a1",
   "metadata": {},
   "source": [
    "질문1. 검색어로 찾은 책 목록을 json 파일로 저장하기\n",
    "*   data/books.json 파일로 저장해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11940e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('../data/books.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(items_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb3b08",
   "metadata": {},
   "source": [
    "질문2. books.json 파일을 Pandas DataFrame로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7aee2e",
   "metadata": {},
   "source": [
    "질문3. 검색어로 찾은 책 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca557353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e856943",
   "metadata": {},
   "source": [
    "질문4. 검색어로 찾은 책 목록 중에서 가격이 2만원 이상인 책만 출력하기\n",
    "*   title, author, discount, pubdate 컬럼만 출력\n",
    "*   가격은 descending(내림차순), index 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6978a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d819268d",
   "metadata": {},
   "source": [
    "질문5. 검색어로 찾은 책 목록 중에서 출판사가 \"인피니티북스\"인 책만 출력하기\n",
    "*   image, description 컬럼은 제외한 모든 컬럼 출력하기\n",
    "*   index는 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac91a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
