{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b63e3",
   "metadata": {},
   "source": [
    "### 1. Daum 뉴스 기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90268913",
   "metadata": {},
   "source": [
    "질문1. 아래의 url에서 뉴스 기사의 링크와 제목을 출력하시오\n",
    "*  경제 뉴스 url = 'https://news.daum.net/economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# dict 타입으로 요청 파라미터 설정\n",
    "# 요청 파라미터 설정: 고정된 주소 뒤에 내가 원하는 조건(카테고리 등)을 유동적으로 조립하기 위해 미리 값을 준비해두는 과정\n",
    "req_param = {\n",
    "    'category': 'economy' \n",
    "}\n",
    "\n",
    "url = 'https://news.daum.net/{category}'.format(**req_param)\n",
    "print(url) \n",
    "\n",
    "# 요청 헤더 설정 why? 프로그램이 아닌 사람처럼 보이게 하기 위함\n",
    "# 개발자 도구 네트워크 Doc의 헤더에서 가져올 수 있음\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests의 get() 함수 호출하기\n",
    "# requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "# url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "# headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "print(type(res)) \n",
    "print(res.status_code) \n",
    "\n",
    "# if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "if res.ok: \n",
    "    # 한글 깨짐 방지\n",
    "    res.encoding = 'utf-8'\n",
    "    # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "    # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "    soup = BeautifulSoup(res.text, 'html.parser') \n",
    "\n",
    "    # 기사 제목과 링크 추출\n",
    "    # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "    # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "    li_tag_list = soup.select(\"ul.list_newsheadline2 li\")\n",
    "\n",
    "    # type: 데이터 형식\n",
    "    # len: 그 안에 든 뉴스 기사가 몇 개인지\n",
    "    print(type(li_tag_list), len(li_tag_list))\n",
    "\n",
    "    # 찾아온 뉴스 꾸러미(li_tag_list)에서 기사를 하나씩 꺼내서(li_tag) 아래 작업을 반복\n",
    "    for li_tag in li_tag_list:\n",
    "\n",
    "        a_tag = li_tag.find('a')\n",
    "        # 해당하는 링크를 가져와서 출력하기\n",
    "        print(a_tag['href'])\n",
    "\n",
    "        #strong_tag = li_tag.select('div.cont_thumb strong.tit_txt')[0]\n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "        # .text: 태그 안의 텍스트만 가져오기\n",
    "        # .strip(): 공백, 줄바꿈 제거\n",
    "        title = strong_tag.text.strip()\n",
    "        print(title)\n",
    "\n",
    "else:\n",
    "    # 응답(response)이 Error이면 status code 출력\n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd751",
   "metadata": {},
   "source": [
    "질문2. 여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n",
    "*   경제 뉴스 url = 'https://news.daum.net/economy'\n",
    "*   사회 뉴스 url = 'https://news.daum.net/society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1673e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics', '국제':'world','문화':'culture',\n",
    "                '생활':'life','IT/과학':'tech','인물':'people','지식/칼럼':'understanding','연재':'series'}\n",
    "\n",
    "def print_news(section_name):\n",
    "\n",
    "    section = section_dict[section_name]\n",
    "\n",
    "    # 요청 Parameter\n",
    "    req_param = {\n",
    "        'section': section\n",
    "    }\n",
    "\n",
    "    url = 'https://news.daum.net/{section}'.format(**req_param)\n",
    "\n",
    "    # f-string: 문자열 안에서 중괄호 { }를 사용해 변수나 계산식을 직접 넣을 수 있음\n",
    "    # print(\"======>\" + url + section_name + \"뉴스\" + \"<======\")\n",
    "    print(f'======> {url} {section_name} 뉴스 <======')\n",
    "\n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "    \n",
    "    # if res.ok: 서버로부터 응답을 제대로 받았는지\n",
    "    if res.ok:\n",
    "        # 한글 깨짐 방지\n",
    "        res.encoding = 'utf-8'\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 기사 제목과 링크 추출\n",
    "        # 긴 html에서 내가 원하는 부분만 가져오기\n",
    "        # ul.list_newsheadline2: 클래스가 list_newsheadline2인 ul 태그 찾기\n",
    "        li_tag_list = soup.select(\"ul.list_newsheadline2 li\")\n",
    "\n",
    "        # 찾아온 뉴스 꾸러미(li_tag_list)에서 기사를 하나씩 꺼내서(li_tag) 아래 작업을 반복\n",
    "        for li_tag in li_tag_list:\n",
    "\n",
    "            a_tag = li_tag.find('a')\n",
    "            # 해당하는 링크를 가져와서 출력하기\n",
    "            print(a_tag['href'])\n",
    "\n",
    "            #strong_tag = li_tag.select('div.cont_thumb strong.tit_txt')[0]\n",
    "            strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "            # .text: 태그 안의 텍스트만 가져오기\n",
    "            # .strip(): 공백, 줄바꿈 제거\n",
    "            title = strong_tag.text.strip()\n",
    "            print(title)\n",
    "\n",
    "    else:\n",
    "        # 응답(response)이 Error이면 status code 출력\n",
    "        print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc1fc",
   "metadata": {},
   "source": [
    "### 2-1. Nate 뉴스 기사 제목 스크래핑하기\n",
    "*   https://news.nate.com/recent?mid=n0100\n",
    "*   최신뉴스, 정치 , 경제, 사회, 세계, IT/과학 6개의 섹션의 뉴스를 출력하는 함수를 생성하여 스크래핑 하기\n",
    "*   뉴스기사의 Image를 출력 하세요\n",
    "*   Image의 도메인이름이 포함된 url과 src 속성의 img 경로를 합치려면 urljoin 함수를 사용하세요\n",
    "*   Image 출력은 Image 클래스와 display 함수를 사용하세요\n",
    "*   img 엘리먼트의 존재 여부를 체크하신 후에 src 속성의 이미지를 경로를 추출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9500e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import Image, display\n",
    "\n",
    "section_dict = {'최신뉴스':100,'정치':200, '경제':300, '사회':400 ,'세계':500, 'IT/과학':600}\n",
    "\n",
    "def print_news(section_num):\n",
    "\n",
    "    section = section_dict[section_num]\n",
    "\n",
    "    # 요청 Parameter\n",
    "    req_param = {\n",
    "        'section': section\n",
    "    }\n",
    "\n",
    "    url = 'https://news.nate.com/recent?mid=n0{section}'.format(**req_param)\n",
    "    \n",
    "    # 요청 헤더 설정\n",
    "    req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # requests의 get() 함수 호출하기\n",
    "    # requests.get(): 이 주소(URL)에 있는 정보를 나에게 보내달라고 요청 하는 역할\n",
    "    # url: \"어디로\" 갈 것인가? e.g. 다음 경제 뉴스 페이지\n",
    "    # headers=req_header: \"어떤 모습으로\" 갈 것인가? 앞서 설명한 것처럼 user-agent를 담아 사람인 척 위장하는 가면을 쓰는 부분\n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # 서버에서 제공한 실제 인코딩 방식으로 강제 설정 (한글 깨짐 방지)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    if res.ok:\n",
    "        # 한글 깨짐 방지\n",
    "        res.encoding = 'euc-kr'\n",
    "        # res.text는 긴 글자 뭉친인데 BeautifulSoup을 거치면 '태그를 찾아줘' 등과 같은 명령어를 쓸 수 있는 soup 객체가 됨\n",
    "        # html.parser: BeautifulSoup에게 html.parser 도구로 분석하라고 알려줌\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 기사 묶음 리스트\n",
    "        tags = soup.select(\"div.postListType.noListTitle div.mlt01\")\n",
    "\n",
    "        # idx: 기사 번호\n",
    "        # div_tag: 뉴스 기사 하나에 해당하는 html 덩어리\n",
    "        # enumerate(tags, 1): tags에서 하나식 꺼내는데 번호표를 1번부터 붙여서 가져옴\n",
    "        for idx, div_tag in enumerate(tags, 1): \n",
    "            print(f'============>> {idx}')\n",
    "\n",
    "            # 기사 덩어리 안에서 링크가 걸린 <a>태그 찾기       \n",
    "            a_tag = div_tag.find('a')\n",
    "            # urljoin(url, ...): 생략된 앞부분 주소를 자동으로 붙여서 완전한 주소로 만들어주는 함수\n",
    "            # 현재 <a>태그에 있는 링크: //news.nate.com/view/20260209n25017?mid=n0100(불완전한 기사 주소)\n",
    "            a_join_url = urljoin(url, a_tag['href'])\n",
    "            print(f'뉴스기사 링크 = {a_join_url}')\n",
    "\n",
    "            # span.ib 안에 있는 이미지를 찾기\n",
    "            img_tag = div_tag.select_one('span.ib img')\n",
    "            # 이미지가 있을 때만 실행하기\n",
    "            if img_tag:\n",
    "                photo_url = urljoin(url, img_tag['src'])\n",
    "                # 웹사이트 소스에서 갓 긁어온 값 (불완전한 주소)\n",
    "                print(img_tag['src'])\n",
    "                # urljoin을 걸쳐서 완벽한 절대주소 값 (완전한 이미지 주소)\n",
    "                print(photo_url)\n",
    "                # 텍스트 주소 형태인 photo_url을 실제 이미지로 변환해서 화면에 보여줌\n",
    "                display(Image(url=photo_url))\n",
    "\n",
    "            # 기사 제목이 들어있는 <h2>태그만\n",
    "            h2_tag = div_tag.select_one('span.tb h2.tit')\n",
    "            # .text: <h2>...</h2>태그는 버리고 그 사이에 적힌 제목만\n",
    "            title = h2_tag.text\n",
    "            print(title)\n",
    "\n",
    "    else:\n",
    "        # 응답(response)이 Error이면 status code 출력\n",
    "        print(f'Error Code = {res.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade47db3",
   "metadata": {},
   "source": [
    "### 2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차번호(no),회차의URL(url) 을 입력으로 받는 함수를 선언\n",
    "*   아래와 같이 호출\n",
    "\n",
    "    ```download_one_episode('일렉시드',341,'https://comic.naver.com/webtoon/detail?titleId=717481&no=341&week=wed')```\n",
    "    \n",
    "*   img\\일렉시드\\341 디렉토리가 생성되며, 그 디렉토리 아래에 웹툰 image들이 다운로드 되도록 해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e799d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318fe264",
   "metadata": {},
   "source": [
    "### 2-3. 하나의 네이버 웹툰과 여러개의 회차에 대한 Image 다운로드 하기\n",
    "*   하나의 웹툰의 제목(title)과 회차를 알 수 있는 url을 입력으로 받는 함수를 선언\n",
    "\n",
    "    ````def download_all_episode(title, episode_url):````\n",
    "\n",
    "*   하나의 웹툰에 대한 1Page의 20 회차의 image를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4992ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "839c8ee4",
   "metadata": {},
   "source": [
    "### 3-1. 네이버 책 검색 API 호출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b276abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 현재 폴더에 있는 .env 파일을 찾아 내용을 메모리 불러오기\n",
    "load_dotenv()\n",
    "# os.getenv()를 통해 .env 안에 적힌 '변수명'으로 실제 값 가져오기\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "headers = {\n",
    "    'X-Naver-Client-Id': CLIENT_ID,\n",
    "    'X-Naver-Client-Secret': CLIENT_SECRET\n",
    "}\n",
    "\n",
    "def search_books(query):\n",
    "\n",
    "    # query string 문자열을 dict로 선언 \n",
    "    payload = {\n",
    "        'query': query, # 파이썬\n",
    "        'display': 100,\n",
    "        'sort': 'sim'\n",
    "    }\n",
    "\n",
    "    url = 'https://openapi.naver.com/v1/search/book.json'\n",
    "\n",
    "    # API 요청 \n",
    "    # requests get(url, params, headers) 요청\n",
    "    res = requests.get(url, params=payload, headers=headers)\n",
    "\n",
    "    if res.ok:\n",
    "        # json()함수로 응답 결과 가져오기\n",
    "        return res.json()['items']\n",
    "    else:\n",
    "        print(f\"Error: {res.status_code}\")\n",
    "        return []\n",
    "    \n",
    "items_data = search_books('파이썬')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e8a1",
   "metadata": {},
   "source": [
    "질문1. 검색어로 찾은 책 목록을 json 파일로 저장하기\n",
    "*   data/books.json 파일로 저장해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11940e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('../data/books.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(items_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb3b08",
   "metadata": {},
   "source": [
    "질문2. books.json 파일을 Pandas DataFrame로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# JSON 파일을 데이터프레임으로 불러오기\n",
    "data = pd.read_json('../data/books.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7aee2e",
   "metadata": {},
   "source": [
    "질문3. 검색어로 찾은 책 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca557353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display()함수: 표 형식으로 데이터 출력\n",
    "# .head() 상위 5개 데이터 확인\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e856943",
   "metadata": {},
   "source": [
    "질문4. 검색어로 찾은 책 목록 중에서 가격이 2만원 이상인 책만 출력하기\n",
    "*   title, author, discount, pubdate 컬럼만 출력\n",
    "*   가격은 descending(내림차순), index 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6978a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc()함수: 이름을 기준으로 행과 열을 선택\n",
    "# loc[Row 선택, Column 선택]\n",
    "# ascending=False: 내림차순(descending)\n",
    "# .reset_index(): 현재 설정된 인덱스(행 이름)를 버리고, 다시 0부터 시작하는 숫자 번호표(0, 1, 2...)로 초기화\n",
    "data.loc[data['discount'] >= 20000 , ['title','author','discount','pubdate']].sort_values(by='discount', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819268d",
   "metadata": {},
   "source": [
    "질문5. 검색어로 찾은 책 목록 중에서 출판사가 \"인피니티북스\"인 책만 출력하기\n",
    "*   image, description 컬럼은 제외한 모든 컬럼 출력하기\n",
    "*   index는 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac91a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc()함수: 이름을 기준으로 행과 열을 선택\n",
    "# loc[Row 선택, Column 선택]\n",
    "# .reset_index(): 현재 설정된 인덱스(행 이름)를 버리고, 다시 0부터 시작하는 숫자 번호표(0, 1, 2...)로 초기화\n",
    "data.loc[data['publisher'] == '인피니티북스' , ['title','link','author','discount','publisher','pubdate','isbn']].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
